


## 1) 전체 아키텍처 한눈에

```css
[사용자(Vue)] ──> [API Gateway(Laravel)]
                  │
                  ├─ /chat  ─────> [FastAPI 챗 라우터] ──> [LLM 서버(vLLM/SGLang)]
                  │                                  ├─[임베딩/벡터DB(Qdrant/Milvus)]
                  │                                  ├─[도구: 주문/티켓 조회 API]
                  │                                  └─[필터/레이트리밋/로깅]
                  └─ /admin ───> FAQ/지식·로그 관리 콘솔
```
* 핵심 구성: Llama 모델 + RAG(사내 FAQ/가이드/티켓검색) + 툴(주문·반품 조회) + 안전장치(PII 마스킹/욕설필터)
* 권장 하드웨어: 7B~8B(지식+도구 기반) 시작 시 RTX 4090/5090 단일로 충분. 동시성↑ 또는 30B급 원하면 다GPU 확장.


## 2) 모델·서빙·임베딩 고르기

* 모델(예시): Llama 3.x 8B Instruct(한국어 지원 괜찮음) → 7B/8B로 시작, 필요 시 13B/30B.
* 서빙(택1)
 - vLLM: 고동시성·낮은 지연. OpenAI 호환 API 노출 쉬움.
 - SGLang: 함수호출/스트리밍 강점, 경량 배포 용이.
* 임베딩 모델(한국어 다국어): bge-m3 또는 e5-mistral(문서 검색 성능 우수).
* 벡터DB: Qdrant(설치·운영 쉬움) 또는 Milvus.

## 3) 데이터 파이프라인(RAG)

1. 지식 수집: FAQ, 매뉴얼, 약관, CS 스크립트, 과거 티켓(민감정보 제거)
2. 청크화: 300~600 토큰 단위로 문서 분할(제목·섹션 태그 보존)
3. 임베딩 생성 → 벡터DB 업로드(메타데이터: 카테고리/버전/날짜)
4. 검색 전략: hybrid(텍스트 BM25 + 벡터), top-k 3~5, rerank 선택
5. 프롬프트 구성: 시스템(역할/어투/금칙), 컨텍스트(검색 결과), 유저 질문, 포맷 지시(한글/JSON 등)

<b>샘플 시스템 프롬프트(요약형)</b>
```diff
당신은 한국어 고객센터 상담원 AI입니다. 
원칙:
- 확실한 정보만 답변. 불확실하면 추가 질문.
- 개인정보, 내부 기밀, 결제정보는 노출 금지.
- 결과는 짧고 명확하게, 필요한 경우 bullet.
- 최종 답변 맨 아래에 참조 문서 제목과 날짜를 나열.
```


## 4) 도구(툴) 연결: 주문·반품·티켓 조회

* 함수 호출/툴로 백엔드 REST를 연결해 실시간 정보를 보강.
* 예: getOrderStatus(order_no), openTicket(user_id, category, message), getReturnPolicy(product_id)


## 5) 백엔드 스켈레톤

### (A) vLLM 서버 띄우기 (단일 GPU)
```bash
# 예) vLLM OpenAI 호환 서버
pip install vllm "transformers>=4.43" accelerate
python -m vllm.entrypoints.openai.api_server \
  --model meta-llama/Llama-3-8B-Instruct \
  --dtype auto --gpu-memory-utilization 0.90 \
  --max-model-len 8192 --port 8008
# OpenAI 스타일: http://localhost:8008/v1/chat/completions
```

### (B) FastAPI: 라우터 + RAG + 툴콜 기본형
```py
 from fastapi import FastAPI
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchValue
import httpx, os

# --- 설정 ---
QDRANT_URL = os.getenv("QDRANT_URL", "http://localhost:6333")
COLLECTION = "kb"
EMB_MODEL  = os.getenv("EMB_MODEL", "BAAI/bge-m3")

VLLM_BASE  = os.getenv("VLLM_URL", "http://localhost:8008/v1")
LLM_MODEL  = os.getenv("LLM_MODEL", "meta-llama/Llama-3-8B-Instruct")
LLM_KEY    = os.getenv("OPENAI_API_KEY", "dummy")  # vLLM은 없어도 됨

SYSTEM_PROMPT = """당신은 한국어 고객센터 상담원 AI입니다.
- 제공된 컨텍스트에 근거해 간결하고 정확히 답하세요.
- 모르면 모른다고 하고 필요한 정보를 되물으세요.
- 마지막에 '참조'로 문서 제목/날짜를 1~3개 표시하세요.
"""

# --- 준비 ---
app = FastAPI()
embedder = SentenceTransformer(EMB_MODEL)
qdrant = QdrantClient(url=QDRANT_URL)

class ChatReq(BaseModel):
    message: str
    top_k: int = 4

def retrieve(query: str, top_k: int = 4, lang: str = "ko"):
    # [1] 임베딩: 질문을 임베딩 모델로 벡터화
    qvec = embedder.encode(query, normalize_embeddings=True).tolist()

    # [2] 벡터 검색: Qdrant에서 유사도 top-k 검색
    hits = qdrant.search(
        collection_name=COLLECTION,
        query_vector=qvec,
        limit=top_k,
        query_filter=Filter(must=[FieldCondition(key="lang", match=MatchValue(value=lang))])
    )

    # [3] 문서 추출: payload(제목/날짜/본문)로 컨텍스트 구성
    ctx_blocks, refs = [], []
    for h in hits:
        p = h.payload
        title, date, content = p.get("title"), p.get("date"), p.get("content")
        ctx_blocks.append(f"[제목]{title} [날짜]{date}\n{content}")
        refs.append({"title": title, "date": date})
    context = "\n\n---\n\n".join(ctx_blocks)
    return context, refs

async def call_llm(messages):
    # [4] LLM 호출: system + user(컨텍스트 포함) 메시지로 vLLM(OpenAI 호환) 호출
    async with httpx.AsyncClient(timeout=90) as client:
        r = await client.post(
            f"{VLLM_BASE}/chat/completions",
            headers={"Authorization": f"Bearer {LLM_KEY}"},
            json={
                "model": LLM_MODEL,
                "messages": messages,
                "temperature": 0.2,
                "max_tokens": 512,
                "stream": False,
            },
        )
        r.raise_for_status()
        return r.json()["choices"][0]["message"]["content"]

@app.post("/chat")
async def chat(req: ChatReq):
    # 단계 1~3 실행: 검색 및 컨텍스트 준비
    context, refs = retrieve(req.message, req.top_k)

    # 단계 4 준비: system + user 메시지 구성
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": f"[사용자 문의]\n{req.message}\n\n[검색 컨텍스트]\n{context}"},
    ]

    # 단계 4 실행: LLM 호출
    answer = await call_llm(messages)

    # 최종 응답 반환
    return {"answer": answer, "refs": refs}

```

### (C) Laravel(게이트웨이)에서 FastAPI 프록시
```php
// routes/api.php
Route::post('/cs/chat', [CsChatController::class, 'chat']);

// app/Http/Controllers/CsChatController.php
public function chat(Request $req) {
    $resp = Http::timeout(60)->post(env('CS_FASTAPI_URL').'/chat', [
        'user_id' => auth()->id(),
        'message' => $req->input('message'),
        'top_k'   => $req->input('top_k', 4)
    ]);
    return response()->json($resp->json());
}
```

### (D) Vue 3 초간단 위젯
```vue
<template>
  <div class="p-4 max-w-xl mx-auto">
    <div v-for="(m,i) in msgs" :key="i" class="mb-2">
      <div v-if="m.role==='user'">🙋 {{ m.content }}</div>
      <div v-else>🤖 {{ m.content }}</div>
    </div>
    <input v-model="q" @keyup.enter="send" placeholder="무엇을 도와드릴까요?" class="border p-2 w-full"/>
  </div>
</template>

<script setup>
import { ref } from 'vue'
const q = ref('환불 규정 알려줘')
const msgs = ref([])

async function send() {
  const msg = q.value.trim()
  if (!msg) return
  msgs.value.push({role:'user', content: msg})
  q.value = ''
  const r = await fetch('/api/cs/chat', {
    method:'POST',
    headers:{'Content-Type':'application/json'},
    body: JSON.stringify({message: msg})
  })
  const j = await r.json()
  msgs.value.push({role:'assistant', content: j.answer})
}
</script>
```

## 6) 안전·정책(필수 체크리스트)
* PII 마스킹: 전화/주민번호/카드번호 정규식 마스킹
* 권한: 주문·티켓 조회는 로그인 세션/토큰 검증 후만 호출(툴콜도 서버측 검증)
* 금칙어·욕설 필터: 응답 전후 필터링(간단 규칙+옵션으로 분류모델)
* 로깅: 질문/답변/사용 컨텍스트(문서ID) 저장 + 재현성 확보
* 레이트 리밋/과금: IP·토큰별 TPS 제한, 장문 응답 길이 제한
* 감사추적: 툴 호출 파라미터·응답 보관(개인정보는 해시/토큰화)


## 7) 성능·비용·규모화 팁
* 7B/8B + RAG로 시작 → 응답 품질 충분 + 지연 0.8~1.5s 수준 achievable(서빙엔진/프롬프트 최적화 시)
* 동시성↑: vLLM(스케줄러/텐서 병합) + KV cache offload
* 캐시: 동일 FAQ는 결과 캐시(질문 정규화)
* 모니터링: 응답시간, 토큰사용, 실패율, 도구 실패율 대시보드


## 8) 파인튜닝(선택)
* LoRA/QLoRA로 사내 톤&규정 주입(7B 기준 24GB VRAM로 충분)
* 데이터: 우수 상담 답변 로그를 대화 형식으로 정제(입력=질문/상황, 출력=최적 답변)
* 검증: 오답/환각 테스트셋 운영(“거짓 답변 금지” 룰)


